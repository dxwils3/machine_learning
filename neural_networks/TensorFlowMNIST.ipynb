{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is taken from the TensorFlow tutorial [expert example](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html).  The purpose here is to create an [MNIST](http://yann.lecun.com/exdb/mnist/) classifier.  I tried to remove all the magic numbers in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as id\n",
    "mnist = id.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we build a simple, fully connected neural network and see that it accurately classifies about 91% of the handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "#inputs and outputs\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# weights and biases are defined, all initialized to zero\n",
    "sess.run(tf.initialize_all_variables())\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "# activation function (softmax as opposed to logistic)\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "# this isn't the cross-entropy function I saw in Nielsen's overview, but seems to be standard.\n",
    "# perhaps try this with nielsen's as well\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# automatically computes derivatives\n",
    "# learning rate is 0.01\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    batch = mnist.train.next_batch(50) # training with batches of 50 from the training data\n",
    "    train_step.run(feed_dict={x: batch[0], y_:batch[1]}) # placeholders must always be fed\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) # argmax returns the index of the largest value, i.e. the predicted number\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "# this is the accuracy on the test data after training\n",
    "# eval on a tensor is the same as passing the tensor to sess.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a deep neural network.  This ANN has the following layers:\n",
    "1. Convolution layer (with pooling)\n",
    "2. Convolution layer (with pooling)\n",
    "3. Dense layer (with dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.84\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.94\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.96\n",
      "step 1000, training accuracy 0.96\n",
      "test accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "# i.e. no negatives\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "conv1_filters = 50\n",
    "conv2_filters = 70\n",
    "filter_size = 5\n",
    "output_length = 10\n",
    "batch_size = 50\n",
    "fully_connected_length = int((image_height + filter_size - 1) * (image_width + filter_size - 1))\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# all bias variables initialized to 0.1\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# this is a convolution layer\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv1 = weight_variable([filter_size, filter_size, 1, conv1_filters])\n",
    "b_conv1 = bias_variable([conv1_filters])\n",
    "x_image = tf.reshape(x, [-1,image_height,image_width,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "W_fc1 = weight_variable([int(image_height/4 * image_width/4 * conv2_filters), \n",
    "                         fully_connected_length])\n",
    "b_fc1 = bias_variable([fully_connected_length])\n",
    "W_conv2 = weight_variable([filter_size, filter_size, conv1_filters, conv2_filters])\n",
    "b_conv2 = bias_variable([conv2_filters])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, int(image_height/4*image_width/4*conv2_filters)])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = weight_variable([fully_connected_length, output_length])\n",
    "b_fc2 = bias_variable([output_length])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(1001):\n",
    "  batch = mnist.train.next_batch(batch_size)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
